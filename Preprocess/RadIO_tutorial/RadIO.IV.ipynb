{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training models for pulmonary nodule detection on LUNA16 dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial you will learn how to train models on popular [LUNA16](http://luna16.grand-challenge.org) competition dataset (scans from [LIDC-IDRI])(https://wiki.cancerimagingarchive.net/display/Public/LIDC-IDRI).\n",
    "\n",
    "Notebook is divided into four parts:\n",
    "1. Preparation of dataset containing CT-scans crops\n",
    "2. Training a ready-made model from recommended RadIO model zoo, e.g. [**Keras3DUnet**](https://analysiscenter.github.io/radio/api/keras_3dunet.html)\n",
    "3. Training a popular model from dataset/models zoo, e.g. [**V-Net**](https://analysiscenter.github.io/dataset/api/dataset.models.tf.vnet.html)\n",
    "4. Integrating custom model with RadIO framework"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before proceeding to the plan, we will make some standard imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append('../')\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part I. Prepare dataset of crops"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Traininig a neural network on whole CT-scans is not yet practically approachable, as they weight too much (300-500mb+). Therefore using smaller 3D parts, crops, would be a natural way to tackle volumetric information. Cropping parts with/without nodules will also facilitate better sampling and allow to tune balance of positive/negative examples in batch more accurately. RadIO contains ready-made preprocessing pipeline which will create a dataset with crops. You may use LUNA16 challenge dataset in MetaImage format to run this tutorial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: You will also need **annotations.csv** file with annotation in format similar to file provided for the LUNA16 dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import radio\n",
    "from radio import batchflow as bf\n",
    "from radio import CTImagesMaskedBatch as CTIMB\n",
    "\n",
    "# read annotation\n",
    "nodules = pd.read_csv('//vmware-host/Shared Folders/OneDrive/Final Project/Dataset/LUNA-16/annotations.csv')\n",
    "\n",
    "subset_path = Path('//vmware-host/Shared Folders/OneDrive/Final Project/Dataset/LUNA-16/subset/')\n",
    "mhd_files = [str(subset_path / f) for f in os.listdir(subset_path) if f.endswith('.mhd')]\n",
    "\n",
    "# create index and dataset\n",
    "lunaix = bf.FilesIndex(path=mhd_files, no_ext=True)\n",
    "lunaset = bf.Dataset(index=lunaix, batch_class=CTIMB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you need to split your dataset into train/test/validation parts, you can use `.split` method. For example, splitting dataset into 90% for train, 10% for test and 0% for validation will be:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-0696924fb4ac>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mlunaset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0.7\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\radio\\batchflow\\batchflow\\base.py\u001b[0m in \u001b[0;36msplit\u001b[1;34m(self, shares, shuffle)\u001b[0m\n\u001b[0;32m    142\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    143\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 144\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcreate_subset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    145\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtest\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    146\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtest\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcreate_subset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtest\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\radio\\batchflow\\batchflow\\dataset.py\u001b[0m in \u001b[0;36mcreate_subset\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m     47\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mcreate_subset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m         \u001b[1;34m\"\"\" Create a dataset based on the given subset of indices \"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 49\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_dataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     50\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mcreate_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_indices\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpos\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\radio\\batchflow\\batchflow\\dataset.py\u001b[0m in \u001b[0;36mfrom_dataset\u001b[1;34m(cls, dataset, index, batch_class)\u001b[0m\n\u001b[0;32m     27\u001b[0m         \u001b[1;33m(\u001b[0m\u001b[0musually\u001b[0m \u001b[0ma\u001b[0m \u001b[0msubset\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0msource\u001b[0m \u001b[0mdataset\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m         \"\"\"\n\u001b[1;32m---> 29\u001b[1;33m         \u001b[1;32mif\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mbatch_class\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mbatch_class\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatch_class\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mcls\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_is_same_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     30\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m         \u001b[0mbcl\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch_class\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mbatch_class\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatch_class\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\radio\\batchflow\\batchflow\\dataset.py\u001b[0m in \u001b[0;36m_is_same_index\u001b[1;34m(index1, index2)\u001b[0m\n\u001b[0;32m     42\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_is_same_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 44\u001b[1;33m                \u001b[0mindex1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mindex2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[1;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     45\u001b[0m                \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindices\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mindex2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "lunaset.split(0.7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now original dataset is divided into train and test parts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(lunaset.train), len(lunaset.test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we need to write down preprocessing workflow. \n",
    "\n",
    "Function **split_dump** from `radio.pipelines` returns basic workflow, including normalizing [**Hounsfield Units**](https://en.wikipedia.org/wiki/Hounsfield_scale), resizing all scans to fixed scans and in at same time unifying its spacing to desired one. After preprocessing **split_dump** function samples smaller crops of desired shape **(32, 64, 64)** with and without cancer nodules. Workflow will also create masks based on nodules location and dump all components, e.g. `masks`, `images`, into fast [**blosc**](http://blosc.org/pages/blosc-in-depth/) format.\n",
    "\n",
    "Crops with cancer nodules will be dumped to `cancer_path` and crops without any nodules will be dumped to `non_cancer_path`. It allows managing balance of positive/negative examples in batch later during training.\n",
    "\n",
    "You may want to pass additioanl paramteres as `kwargs`: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from radio.pipelines import split_dump\n",
    "\n",
    "SPACING = (1.7, 1.0, 1.0)  # spacing of scans after spacing unification\n",
    "SHAPE = (400, 512, 512)  # shape of scans after spacing unification\n",
    "PADDING = 'reflect'  # 'reflect' padding-mode produces the least amount of artefacts\n",
    "METHOD = 'pil-simd'  # robust resize-engine\n",
    "\n",
    "kwargs_default = dict(shape=SHAPE, spacing=SPACING, padding=PADDING, method=METHOD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "crop_pipeline = split_dump(cancer_path='/data/lunaset_split/train/cancer/', \n",
    "                           non_cancer_path='/data/lunaset_split/train/ncancer/',\n",
    "                           nodules=nodules, fmt='raw', nodule_shape=(32, 64, 64),\n",
    "                           batch_size=20, **kwargs_default)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pass dataset directly to pipeline and run it, for example for `lunaset.train` sub-dataset:\n",
    "\n",
    "(Note, it may take some time, so better grab a coffee)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "(lunaset.train >> crop_pipeline).run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may want to prepare test part in similar way, but do not forget to change `cancer_path` and `non_cancer_path`. In this tutorial we will skip it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part II. Training a ready-to-use model implemented in Keras: 3D U-Net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**RadIO** contains a small but growing zoo of models in Keras and Tensorflow. In this part you will perform training a 3D U-Net architecture for pulmonary nodule segmentation using dataset of CT-scans crops you prepared earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DIR_CANCER = '/data/lunaset_split_new/train/cancer/*'\n",
    "DIR_NCANCER = '/data/lunaset_split_new/train/ncancer/*'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to preprocessing, let's build a training pipeline. This time, you will need 2 datasets, one for crops with nodules and another without them, that's why they were dumped in different folders. Remeber, that masks and some other special information is also dumped, so it can be easily loaded on request."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all, we create two indices and two datasets corresponding to them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cix = bf.FilesIndex(path=DIR_CANCER, dirs=True)\n",
    "ncix = bf.FilesIndex(path=DIR_NCANCER, dirs=True)\n",
    "\n",
    "cancerset = bf.Dataset(index=cix, batch_class=CTIMB)\n",
    "ncancerset = bf.Dataset(index=ncix, batch_class=CTIMB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How can we use both datasets in a same time during training? Package **dataset** allows to `merge` several pipelines into one requiring only specification of **batch_size** parameter for each of them. Say, we would want take 4 crops with nodules and 4 without them, in total batch_size of 8 crops of (32, 64, 64) size. Let's import a ready made pipeline **combine_crops** for this problem:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from radio.pipelines import combine_crops\n",
    "from utils import show_slices # function for plotting batch masks and images\n",
    "combine_pipeline = combine_crops(cancerset, ncancerset, batch_sizes=(4, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Here we generate one batch with 4 cancer crops and 4 non-cancer crops\n",
    "batch = combine_pipeline.next_batch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look inside of generated batch. To do that we use `show_slices` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_slices(batch, scan_indices=4, ns_slice=16, clims=[(0, 255), (0, 1)],\n",
    "            components=('images', 'masks'), grid=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next step is the model: import and desciption of configuration dict. Model requires specification of input crops and masks sizes (will be same) as well as Optimizer and Loss function; e.g. use Adam for training and Dice loss funciton for segmentation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from radio.models import Keras3DUNet\n",
    "from radio.models.keras.losses import dice_loss\n",
    "\n",
    "unet_config = dict(\n",
    "    input_shape = (1, 32, 64, 64),\n",
    "    num_targets = 1,\n",
    "    loss= dice_loss\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After that, let's add one more part to training pipeline specifying model and training params by chaining `.init_model` and `.train_model` pipeline methods:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from radio.batchflow import F\n",
    "\n",
    "train_unet_pipeline = (\n",
    "    combine_crops(cancerset, ncancerset, batch_sizes=(4, 4))\n",
    "    .init_model(\n",
    "        name='3dunet', model_class=Keras3DUNet,\n",
    "        config=unet_config, mode='static'\n",
    "    )\n",
    "    .train_model(\n",
    "        name='3dunet',\n",
    "        x=F(CTIMB.unpack, component='images', data_format='channels_first'),\n",
    "        y=F(CTIMB.unpack, component='masks', data_format='channels_first')\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here `train_model` employ custom **CTIMB.unpack** method that helps to unpack specific components from batch and put them into desired alias of keras network: `images` component goes to `x` and `masks` into `y`.\n",
    "\n",
    "If you want to try a keras network for classification, you may simply pass component for y's unpack to `classification_targets`. That way, unpack would transform masks into binary classification (`nodule` vs. `non-nodule`) target based on threshold of amount of cancer voxels inside the crop to label `nodule`. See [documentation](https://analysiscenter.github.io/radio/intro/models.html) for details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, the moment of truth:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_unet_pipeline.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After execution is finished, you may `.save` keras model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "keras_unet = train_unet_pipeline.get_model_by_name('3dunet')\n",
    "keras_unet.save('/path/to/keras_3d_unet/weights')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you later would like to access it, that would be easy by adding `path` in `model_config` and redefining and running same pipeline again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "unet_config.update({'path': '/path/to/keras_3d_unet/weights'})\n",
    "\n",
    "train_unet_pipeline = (\n",
    "    combine_crops(cancerset, ncancerset, batch_sizes=(4, 4))\n",
    "    .init_model(\n",
    "        name='3dunet', model_class=Keras3DUNet,\n",
    "        config=unet_config, mode='static'\n",
    "    )\n",
    "    .train_model(\n",
    "        name='3dunet',\n",
    "        x=F(CTIMB.unpack, component='images', data_format='channels_first'),\n",
    "        y=F(CTIMB.unpack, component='masks', data_format='channels_first')\n",
    "    )\n",
    ")\n",
    "\n",
    "train_unet_pipeline.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: we have created a new pipeline instance to avoid model names collision when init_model method is called second time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3. Training a model from radio.dataset.models zoo: V-Net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset library used by RadIO contains a zoo of many popular model architecture in tensorflow. In this part you will learn to train V-Net (volumetric u-net) for same task with same preprocessing pipeline. All you need is to define config of dataset's v-net model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from radio.batchflow.models.tf import VNet\n",
    "# We use Tversky loss here cause it performs slightly better than dice\n",
    "from radio.models.tf.losses import tversky_loss\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vnet_config = {\n",
    "    'inputs': dict(\n",
    "        images={'shape': (32, 64, 64, 1)},\n",
    "        labels={'name': 'targets', 'shape': (32, 64, 64, 1)}\n",
    "    ),\n",
    "    'loss': tversky_loss,\n",
    "    'optimizer': 'Adam',\n",
    "}\n",
    "vnet_config['input_block/inputs'] = 'images'\n",
    "vnet_config['head/layout'] = 'cna'\n",
    "vnet_config['head/activation'] = tf.nn.sigmoid\n",
    "vnet_config['head/kernel_size'] = 1\n",
    "vnet_config['head/num_classes'] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Subpackage **dataset.models** has highly parametrized architectures that allow to perform experiments by combining different paramteres without changing model's code. It greatly facilitates clear and reproducible experiment settings.\n",
    "\n",
    "In example above you descibed configuration dictionary for V-Net model which will be trained on crops with Adam optimizer, tversky loss. Slight modifications are added to head of the network, last convolutional block would consist of convolution with (1, 1, 1) kernel, 1 filter, following batch normalisation layer and sigmoid activation function. See dataset [documentation](https://analysiscenter.github.io/dataset/intro/tf_models.html) for details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "from radio.batchflow import V, F\n",
    "\n",
    "train_vnet_pipeline = (\n",
    "    combine_crops(cancerset, ncancerset, batch_sizes=(4, 4))\n",
    "    .init_variable('loss_history', init_on_each_run=list)\n",
    "    .init_variable('current_loss', 0)\n",
    "    .init_model(\n",
    "        name='vnet', model_class=VNet,\n",
    "        config=vnet_config, mode='static'\n",
    "    )\n",
    "    .train_model(\n",
    "        name='vnet', fetches='loss', save_to=V('current_loss'), mode='w',\n",
    "        feed_dict={'images': F(CTIMB.unpack, component='images'),\n",
    "                   'targets': F(CTIMB.unpack, component='masks')}\n",
    "    )\n",
    "    .print(\"Current loss:\")\n",
    "    .print(V('current_loss'))\n",
    "    .update_variable('loss_history', value=V('current_loss'), mode='a')\n",
    "    .call(lambda x: clear_output(wait=True))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_vnet_pipeline.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize loss changes through learning process:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_history = (\n",
    "    pd.Series(train_vnet_pipeline.get_variable('loss_history'))\n",
    "    .rolling(32)\n",
    "    .mean()\n",
    "    .transform(lambda x: -x)\n",
    ")\n",
    "\n",
    "plt.figure(figsize=(7, 5))\n",
    "loss_history.plot(grid=True)\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Tversky loss')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usually, it's quiet usefull to have a look on predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vnet_model = train_vnet_pipeline.get_model_by_name('vnet')\n",
    "predicted_masks = vnet_model.predict(feed_dict={'images': batch.unpack('images')})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "True mask coupled with CT-crop looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_slices(batch, scan_indices=4, ns_slice=16, clims=[(0, 255), (0, 1)],\n",
    "            components=('images', 'masks'), grid=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below you can see predicted mask coupled with CT-crop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch.masks = predicted_masks.reshape(-1, 64, 64)\n",
    "show_slices(batch, scan_indices=4, ns_slice=16, clims=[(0, 255), (0, 1)],\n",
    "            components=('images', 'masks'), grid=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Part 4. Writing custom tensorflow model compatible with the same pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part you will see, how to train your own models using RadIO framework. The easiest way would be to write your model in tensorflow or keras and use `.import_model()` in pipeline. Let's implement a simple 3D convolutional encoder-decoder architecture via tensorflow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all, you need to keep in mind that your custom model must be compatible with `pipeline`'s model specific methods. An easy way to provide this would be to inherit `BaseModel` class and redefine its **`build`** method for building model's graph in tensorflow, **train** method as it will be used by pipeline's **.train_model** and **predict** method to be able to call pipeline's **.predict_model**. Additionaly, you can provide implementation for save and load methods. See documentation of [BaseModel](https://analysiscenter.github.io/dataset/api/dataset.models.html#dataset.models.BaseModel) to make sense of its possibilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from radio.batchflow.models import BaseModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class TFCustomModel(BaseModel):\n",
    "\n",
    "    def __init__(self, config=None, *args, **kwargs):\n",
    "        self.graph = tf.Graph()\n",
    "        with self.graph.as_default():\n",
    "            crop_shape = self.get('crop_shape', config=self.config, default=(32, 64, 64, 1))\n",
    "            self.images = tf.placeholder(shape=(None, *crop_shape), dtype=tf.float32)\n",
    "            self.masks = tf.placeholder(shape=(None, *crop_shape), dtype=tf.float32)\n",
    "\n",
    "        self.predictions = None\n",
    "        self.loss = None\n",
    "        self.train_step = None\n",
    "        self.sess = None\n",
    "\n",
    "        super().__init__(*args, **kwargs)\n",
    "    \n",
    "    def encoder_block(self, input_tensor, filters, name):\n",
    "        with tf.variable_scope(name):\n",
    "            x = tf.layers.conv3d(input_tensor, filters, (3, 3, 3), name='Conv_1',\n",
    "                                 padding='same', activation=tf.nn.relu)\n",
    "            x = tf.layers.conv3d(x, filters, (3, 3, 3), name='Conv_2',\n",
    "                                 padding='same', activation=tf.nn.relu)\n",
    "            x = tf.layers.max_pooling3d(x, pool_size=(2, 2, 2),\n",
    "                                        strides=(2, 2, 2), padding='same')\n",
    "        return x\n",
    "    \n",
    "    def decoder_block(self, input_tensor, filters, name):\n",
    "        with tf.variable_scope(name):\n",
    "            x = tf.layers.conv3d(input_tensor, filters, (3, 3, 3), name='Conv_1',\n",
    "                                 padding='same', activation=tf.nn.relu)\n",
    "            x = tf.layers.conv3d(x, filters, (3, 3, 3), name='Conv_2',\n",
    "                                 padding='same', activation=tf.nn.relu)\n",
    "            x = tf.layers.conv3d_transpose(x, filters, (2, 2, 2), strides=(2, 2, 2),\n",
    "                                           use_bias=False, padding='same')\n",
    "        return x\n",
    "        \n",
    "    def build(self, *args, **kwargs):\n",
    "        filters = self.get('filters', config=self.config, default=(32, 64, 128, 256))\n",
    "        with self.graph.as_default():\n",
    "            x = self.images\n",
    "            for i, f in enumerate(filters):\n",
    "                x = self.encoder_block(x, f, \"EncoderBlock_\" + str(i))\n",
    "            \n",
    "            for i, f in enumerate(filters[::-1]):\n",
    "                x = self.decoder_block(x, f, \"DecoderBlock_\" + str(i))\n",
    "\n",
    "            self.predictions = tf.layers.conv3d(x, 1, (1, 1, 1),\n",
    "                                                padding='same',\n",
    "                                                name='predictions')\n",
    "            \n",
    "            self.loss = tf.losses.sigmoid_cross_entropy(self.masks, self.predictions)\n",
    "            self.train_step = tf.train.AdamOptimizer().minimize(self.loss)\n",
    "            self.sess = tf.Session()\n",
    "            self.sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    def train(self, x, y): \n",
    "        self.sess.run(self.train_step, feed_dict={self.images: x, self.masks: y})\n",
    "    \n",
    "    def predict(self, x):\n",
    "        return self.sess.run(self.predictions, feed_dict={self.images: x})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then you can use `.import_model` just like that, and not forget to provide right arguments (from train method) into `.train_model`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from radio.batchflow import F\n",
    "import numpy as np\n",
    "\n",
    "custom_model = TFCustomModel(config={'crop_shape': (32, 64, 64, 1), 'filters': (32, 64, 128, 256)})\n",
    "\n",
    "train_custom_model_ppl = (\n",
    "    combine_crops(cancerset, ncancerset, batch_sizes=(4, 4))\n",
    "    .import_model(name='custom_model', source=custom_model)\n",
    "    .train_model(\n",
    "        name='custom_model',\n",
    "        x=F(CTIMB.unpack, component='images'),\n",
    "        y=F(CTIMB.unpack, component='masks')\n",
    "    )\n",
    ")\n",
    "\n",
    "train_custom_model_ppl.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "To conclude, in this tutorial you have learnt how to prepare dataset with CT-scans crops for training neural network,\n",
    "trained ready-to use models from **RadIO** and **dataset** and learnt how to integrate custom model with pipeline-specific model methods."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
