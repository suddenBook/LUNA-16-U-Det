{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Diving into scans-preprocessing with RadIO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quick reminder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hello again! This is the second tutorial in the series, dedicated to the lung cancer research with RadIO. In the [first notebook](https://github.com/analysiscenter/radio/blob/master/tutorials/RadIO.I.ipynb) we talked about using RadIO to create a `Dataset` of scans from [LUNA16 competition dataset](https://luna16.grand-challenge.org/). In short, `Dataset` simplifies operating with large datasets that cannot fit in memory (see more [here](https://github.com/analysiscenter/batchflow)). Setting up a `Dataset` takes only several lines of code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from radio.batchflow import FilesIndex, Dataset\n",
    "from radio import CTImagesMaskedBatch\n",
    "\n",
    "LUNA_MASK = '/data/MRT/luna/s*/*.mhd'                     # set glob-mask for scans from Luna-dataset here\n",
    "luna_index = FilesIndex(path=LUNA_MASK, no_ext=True)      # preparing indexing structure\n",
    "luna_dataset = Dataset(index=luna_index, batch_class=CTImagesMaskedBatch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've also seen how easy it is to build simple preprocessing pipelines, that include `load` of data from disk and `resize` of scans to differrent shape:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from radio.batchflow import Pipeline\n",
    "preprocessing = (Pipeline()                      # initialize empty workflow\n",
    "                 .load(fmt='raw')                # add load of scans from MetaImage to the workflow\n",
    "                 .resize(shape=(92, 256, 256)))  # add resize to a shape to the workflow. Nothing is computed here,\n",
    "                                                 # the whole thing is lazy!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "..and generate a batch with 3 loaded and resized scans: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch = (luna_dataset >> preprocessing).next_batch(3)  # pass a batch of luna-scans of size 3 through the workflow "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial we are diving deeper into preprocessing with RadIO. We will cover actions that allow to considerably augment Luna-dataset. `unify_spacing`, `rotate`, `central_crop` are among actions, that help to perform augmentation. What's more, we will cover actions `create_mask` and `fetch_nodules_info`, that will help you to transform with ease [Luna cancer annotations](https://luna16.grand-challenge.org/download/) into *cancerous masks*, **target (Y)** for segmenting nets (think of [VNet](https://arxiv.org/abs/1606.04797))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Augmentation of Luna"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "\n",
    "###  `unify_spacing`: alternative to `resize`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Not infrequently, scans in the dataset have drastically different scales. Take a look at two slices from different scans below: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from utils import show_slices\n",
    "import numpy as np\n",
    "ixs = np.array(['1.3.6.1.4.1.14519.5.2.1.6279.6001.219618492426142913407827034169',\n",
    "                '1.3.6.1.4.1.14519.5.2.1.6279.6001.185154482385982570363528682299'])\n",
    "two_scans_dataset = Dataset(index=luna_index.create_subset(ixs), batch_class=CTImagesMaskedBatch)\n",
    "batch = (two_scans_dataset >> preprocessing).next_batch(2, shuffle=False)   # pass a batch through the workflow\n",
    "show_slices(batch, scan_indices=[0, 1], ns_slice=[39, 57], grid=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, lungs on the right scan are clearly bigger than on the left one. In this case you may say that left and right scans have different **spacings**: the distance between adjacent pixels on the left scan is larger than on the right one. This is demonstrated by grids on both plots: the number of pixels to define 5 cm of the real-world distance is lesser for the left scan. \n",
    "\n",
    "In all, **Luna-dataset** incorporates variability, that is related to peculiarities of data representation, rather than structural differences. Gladly, with action `unify_spacing` from RadIO you can easily make the scans more isotropic, thus eliminating needless variability from the dataset. \n",
    "\n",
    "Let us prepare preprocessing pipeline containing `load` of data from disk and action `unify_spacing`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "preprocessing = (Pipeline()\n",
    "                 .load(fmt='raw')\n",
    "                 .unify_spacing(spacing=(3.5, 1.0, 1.0), shape=(92, 256, 256)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pass a batch through the preprocessing pipeline and check out the result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_unified = (two_scans_dataset >> preprocessing).next_batch(2, shuffle=False)\n",
    "show_slices(batch_unified, scan_indices=[0, 1], ns_slice=[40, 58], grid=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that lungs are much more similar in size, as they have the same scales. The remaining difference in sizes of lungs is explained fully by difference in dimensions of patients, participated in studies.\n",
    "\n",
    "To get an understanding of how `unify_spacing` works, compare the same scan before and after the action:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_slices([batch, batch_unified], scan_indices=[0, 0], ns_slice=[39, 40], grid=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`unify_spacing` cropped the central part of the scan and zoomed it in, so that the new scans' spacing be equal to the supplied one (`[3.5, 1.0, 1.0]` in our case).\n",
    "\n",
    "Essentially, `unify_spacing` is always a combination of *resize* (for manipulation with spacing) and *croping/padding* (for adjustment of shapes). Keep in mind that these parameters **should always be supplied**:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* `shape` - resulting shape of scans, sequence of three elements\n",
    "* `spacing` - resulting spacing of scans, sequence of three elements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "while these can be tweaked for better performance:\n",
    "\n",
    "* `padding` - mode of padding. We recommend to use \"`reflect`\" (default), as it adds the least amount of artefacts. See [doc](https://docs.scipy.org/doc/numpy-1.13.0/reference/generated/numpy.pad.html) of `numpy.pad`\n",
    "* `method` - choose between \"`scipy`\" and \"`pil-simd` (default)\". The first one parallelizes better on systems with large number of cores, but yields more artefacts.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### `unify_spacing` as an augmenting action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, how `unify_spacing` is an augmenting action? The idea is to slightly modify spacings of scans between different runs of `next_batch`. This allows to *controllably* introduce variability in the dataset. Say, you want the spacing to be randomly chosen from small set of predefined options. Assume, these options are"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "spacing_options = [(3.5, 0.5, 0.5), (3.5, 1.0, 1.0), (3.5, 1.5, 1.5), (3.5, 2.0, 2.0)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step is to define *spacing randomizer*, a function that randomly fetches spacing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "spacing_randomizer = lambda *args: spacing_options[np.random.choice(range(len(spacing_options)))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacing_randomizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can now set up the pipeline that generates random spacing for each run of `next_batch`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from radio.batchflow import F                                               # see below the explanation\n",
    "augmenting_pipeline = (Pipeline()                                           # empty workflow\n",
    "                       .load(fmt='raw')                                     # load scans\n",
    "                       .unify_spacing(spacing=F(spacing_randomizer), shape=(92, 256, 256)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that spacing-parameter is set to `F(spacing_randomizer)`. Literally, this means the following: for each batch, passing through `augmenting_pipeline`, the spacing-argument is to be computed by the function `spacing_randomizer`(`F` stands for `F`unction). Pass the same batch through the workflow twice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_1 = (two_scans_dataset >> augmenting_pipeline).next_batch(2, shuffle=False)\n",
    "batch_2 = (two_scans_dataset >> augmenting_pipeline).next_batch(2, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...and see the result by yourself:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_slices([batch_1, batch_2], scan_indices=[0, 0], ns_slice=[40, 40], grid=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### More augmenting actions: `rotate`, `central_crop`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more augmentation you can `rotate` scans on random/specified angle, or crop out central part of scans using `central_crop`. You can of course combine these actions with others from RadIO in a single workflow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "augmenting_pipeline = (Pipeline()                                    # init empty workflow\n",
    "                       .load(fmt='raw')                              # load data from disk\n",
    "                       .resize(shape=(92, 256, 256), method='scipy') # resize with scipy\n",
    "                       .rotate(angle=30, random=False)  # rotate with on 30 degrees about z-axis\n",
    "                       .central_crop(crop_size=(92, 168, 168)))      # perform central_crop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pass a batch through the pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_augmented = (two_scans_dataset >> augmenting_pipeline).next_batch(2, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the effect of augmenting actions is evident, if you put the same batch through the simple preprocessing workflow and compare the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessing_simple = (Pipeline()\n",
    "                        .load(fmt='raw')\n",
    "                        .resize(shape=(92, 256, 256), method='scipy'))\n",
    "batch = (two_scans_dataset >> preprocessing_simple).next_batch(2, shuffle=False)\n",
    "show_slices([batch, batch_augmented], [0, 0], 40, grid=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing a target-output for a segmenting neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you followed the tutorials up to this point, you know how RadIO can help you to\n",
    "* `load` scans from disk\n",
    "* `resize` them to a different shape\n",
    "* ..or make them more isotropic in scales with `unify_spacing`\n",
    "* `rotate` them along slices-axis\n",
    "* and crop out the center part using `central_crop`\n",
    "\n",
    "However, it takes more than a bunch of preprocessing and augmenting operations to train a machine learning algorithm. As it happens, supervised learning algorithms require **target** for the learning procedure. In context of segmentation problem, target is a mask, that highlights an interesting part of an image (be it 2D as in [CIFAR](https://www.cs.toronto.edu/~kriz/cifar.html), or 3D in case of CT-scans). Lung cancer research works with *cancerous masks*: masks, with pixels values lying betweeen **0** and **1**, where **1** indicates cancer. To prepare cancerous masks with RadIO you only need to add two actions in your preprocessing workflow: `fetch_nodules_info` and `create_mask`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: loading info about cancerous tumors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[LUNA16 dataset](https://luna16.grand-challenge.org/) provides targets in the form of a `csv`-table, containing information about cancerous tumors (or *nodules*). Check it out by yourself:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "nodules_df = pd.read_csv('/data/MRT/luna/CSVFILES/annotations.csv')\n",
    "nodules_df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that each line contains information about one cancerous nodule. While `seriesuid`-column indicates the `id` of study (patient) to which the nodule is related, `coordX-Z` contain the real-world coordinates of a nodule. Finally, `diameter_mm` tells us about the real-world size of the tumor.\n",
    "\n",
    "You can put the information from annotations-table in a batch by adding `fetch_nodules_info` to a workflow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "preprocessing = (Pipeline()\n",
    "                 .load(fmt='raw')\n",
    "                 .fetch_nodules_info(nodules_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pass a batch of three items through the workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch = (luna_dataset >> preprocessing).next_batch(3, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and make sure that the `batch` now contains info about nodules:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import get_nodules_pixel_coords\n",
    "get_nodules_pixel_coords(batch)            # get information about nodules from batch\n",
    "                                           # in pixel coords and sizes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Step 2: `create_mask`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To build cancerous masks you only have to add `create_mask` to a workflow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "preprocessing = (Pipeline()\n",
    "                 .load(fmt='raw')\n",
    "                 .fetch_nodules_info(nodules_df)\n",
    "                 .create_mask())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pass a batch through the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch = (luna_dataset >> preprocessing).next_batch(3, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, take a look at the nodules, that are contained in the batch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_nodules_pixel_coords(batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to the first line of the table above, there is a pretty large cancerous nodule on a slice **#113** (`coordZ`=113) of a scan indexed by **1** (`numeric_ix`=1). You can easily see it, by plotting the slice of scan and corresponding mask side by side:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_slices(batch, [1, 1], [113, 113], clims=[(-1200, 300), (0, 1)], components=['images', 'masks'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importantly, you can combine `resize` and `unify_spacing` just fine with `fetch_nodules_info` and `create_mask`, like that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "preprocessing = (Pipeline()\n",
    "                 .load(fmt='raw')                     # still, start with load\n",
    "                 .resize(shape=(124, 298, 298))\n",
    "                 .fetch_nodules_info(nodules_df)\n",
    "                 .unify_spacing(shape=(92, 256, 256), spacing=(3.5, 1.0, 1.0))\n",
    "                 .create_mask()                       # should go after fetch_nodules_info()\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch = (luna_dataset >> preprocessing).next_batch(3, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_nodules_pixel_coords(batch).head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_slices(batch, [1, 1], [32, 32], clims=[(-1200, 300), (0, 1)], components=['images', 'masks'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What's next?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "After reading this tutorial you should be able to chain complex preprocessing and augmenting pipelines, that include `rotate` scans on an angle, `central_crop` and `resize`, `load`-actions, covered in the first tutorial. We've also discussed `unify_spacing`: a very useful alternative to `resize`, that not only equalizes scans' shapes, but also makes them more isotropic, zooming them to the same scale. You also should be able to make use of Luna-annotations and set up cancerous masks using `fetch_nodules_info` and `create_mask`. \n",
    "\n",
    "We also encourage you to stay with us for a little longer and read [3rd](https://github.com/analysiscenter/radio/blob/master/tutorials/RadIO.III.ipynb) and [4th](https://github.com/analysiscenter/radio/blob/master/tutorials/RadIO.IV.ipynb) tutorials. In the [3rd](https://github.com/analysiscenter/radio/blob/master/tutorials/RadIO.III.ipynb) tutorial you'll learn how to use action `sample_nodules` to generate batches of small (much smaller than scans) cancerous and noncancerous crops. In the [4th](https://github.com/analysiscenter/radio/blob/master/tutorials/RadIO.IV.ipynb) tutorial you will use those crops to train a zoo of powerful neural nets (including [`VNet`](https://arxiv.org/abs/1606.04797))."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
