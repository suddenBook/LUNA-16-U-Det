{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "sys.path.append('../')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building batches for training neural nets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Welcome! This is the third tutorial of the [series](https://github.com/analysiscenter/radio/blob/master/tutorials), covering lung cancer research using RadIO. In this tutorial you'll learn about `sample_nodules`-action, that allows to **sample interesting parts of scans** along with cancerous masks, thus considerably augmenting the dataset for a segmenting net. You will also take a look at `Pipelines`-submodule, which contains implementations of several ready-made workflows for preprocessing. If you haven't read the first two tutorials, we encourage you to do that before tackling this one. Anyways, here is a quick reminder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Not-so-quick reminder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When using RadIO, the first step is always to set up a `Dataset` - a structure (`instance` of a `Dataset`-class), indexing set of voluminous ct-scans on disk:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from radio.batchflow import FilesIndex, Dataset\n",
    "from radio import CTImagesMaskedBatch\n",
    "\n",
    "LUNA_MASK = '/data/MRT/luna/s*/*.mhd'                                      # set glob-mask for scans from Luna-dataset here\n",
    "luna_index = FilesIndex(path=LUNA_MASK, no_ext=True)                       # preparing indexing structure\n",
    "luna_dataset = Dataset(index=luna_index, batch_class=CTImagesMaskedBatch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Machine learning-solutions always start with preprocessing. RadIO thinks of preprocessing as of chained sequence of actions -  a `Pipeline`. Each `Pipeline` represents a *plan* of what is going to happen with data, rather than a real computation, and is made of actions, implemented in RadIO ([or by you](https://analysiscenter.github.io/lung_cancer/intro/preprocessing.html#writing-your-own-actions)). E.g., you can set up a simple preprocessing pipeline, including `load` from [Luna dataset](https://luna16.grand-challenge.org/)-format and `resize` to shape **[92, 256, 256]** in a following way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from radio.batchflow import Pipeline                 # the cell executes fast\n",
    "simple_preproc = (Pipeline()                       # we only write a plan\n",
    "                  .load(fmt='raw')                 # no computations here\n",
    "                  .resize(shape=(92, 256, 256)))   # it happens later"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "It might be a good idea to replace `resize` with `unify_spacing`, that not only changes shape of scans, but also zooms them to the same scale:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "other_simple_preproc = (Pipeline()\n",
    "                        .load(fmt='raw')\n",
    "                        .unify_spacing(shape=(92, 256, 256), spacing=(3.5, 1.0, 1.0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also add some data-augmenting actions to your pipeline. E.g., `rotate` of scans or cropping out its central part using `central_crop`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "augmenting_pipeline = (Pipeline()\n",
    "                       .load(fmt='raw')\n",
    "                       .unify_spacing(shape=(92, 256, 256), spacing=(3.5, 1.0, 1.0))\n",
    "                       .rotate(angle=30)\n",
    "                       .central_crop(crop_size=(64, 192, 192)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also get your hands on target for a segmenting net - *cancerous masks*, by adding `fetch_nodules_info` and `create_mask` to a preprocessing pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodules_df = pd.read_csv('/data/MRT/luna/CSVFILES/annotations.csv')\n",
    "nodules_df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "preprocessing = (Pipeline()\n",
    "                 .load(fmt='raw')\n",
    "                 .unify_spacing(shape=(92, 256, 256), spacing=(3.5, 1.0, 1.0))\n",
    "                 .fetch_nodules_info(nodules_df)\n",
    "                 .create_mask())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember, real computation happens **only** when you pass a part of your `Dataset` - a `Batch` through the workflow: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch = (luna_dataset >> preprocessing).next_batch(batch_size=3, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import get_nodules_pixel_coords\n",
    "get_nodules_pixel_coords(batch).head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import show_slices\n",
    "show_slices(batch, 1, [32, 32], components=['images', 'masks'], clims=[(-1200, 300), (0, 1)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At first glance, nothing stops you now from training a segmenting net on full-sized scans. This, however, would require either a **very fast** CPU or a GPU with **a lot** of memory, as few full-sized scans can fit in, say, 8 GB of 1080 Ti-memory. Consider also the following: frequently, scans of Luna-dataset contain more than one cancerous nodule. What a waste to use a scan like that as only one training example! That being said, it may be a good idea to crop out **parts of scans** and train a net on these parts. Action `sample_nodules` allows you to do just that, as you will see in a minute."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Sampling batches of crops with `sample_nodules`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic usage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loaded cancerous masks are necessary requirement for running `sample_nodules`. So, your pipeline should start with something like that:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "preprocessing = (Pipeline()\n",
    "                 .load(fmt='raw')\n",
    "                 .unify_spacing(shape=(128, 256, 256), spacing=(3.5, 1.0, 1.0))\n",
    "                 .fetch_nodules_info(nodules_df)\n",
    "                 .create_mask())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Say, you want to generate batches of **8** crops of shape **(16, 32, 32)**. You need to simply add `sample_nodules` to your `preprocessing`, specifying `batch_size` and `nodule_size`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "crops_sampling_pipeline = preprocessing.sample_nodules(batch_size=8, nodule_size=(16, 32, 32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check out the results by passing a batch of 5 scans through the workflow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "batch_crops = (luna_dataset >> crops_sampling_pipeline).next_batch(5, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_nodules_pixel_coords(batch_crops).head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_slices(batch_crops, 0, 8, components=['images', 'masks'], clims=[(-1200, 300), (0, 1)], grid=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Balancing batches: changing `share` of cancerous crops"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that generated batch contains **8** crops: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(batch_crops)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Still, how much of them contain cancer? Let's find this out by printing number of cancerous pixels in each crop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import num_of_cancerous_pixels\n",
    "num_of_cancerous_pixels(batch_crops)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, **six out of eight** crops contain cancerous pixels. The last two crops are simply cropped from random locations inside the scans. You may want to change the proportion and generate a balanced batch with **half (that is, four out of eight)** of crops being cancerous. For this you only need to set parameter `share` to **0.5**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "crops_sampling_pipeline = preprocessing.sample_nodules(batch_size=8, nodule_size=(16, 32, 32),\n",
    "                                                       share=(0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_crops = (luna_dataset >> crops_sampling_pipeline).next_batch(5, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are only **4** crops containing cancer in a generated batch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_of_cancerous_pixels(batch_crops)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Importantly, generating batches of crops with `sample_nodules` involves setting **two different `batch_size`-parameters**. The first one appears directly in `sample_nodules` action and defines the *size of a resulting batch of crops*. In its turn, the second one is a parameter of `next_batch` and defines the number of scans flowing into the workflow:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "![title](imgs/flow_with_sn.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "When setting parameters `share`, `batch_size` in `next_batch` and `batch_size` in `sample_nodules`, keep in mind that cancerous nodules occur with freqency **~1.3 nodules per scan** in Luna-dataset. That is, to generate batch of crops with **10** cancerous nodules, you should take not less than **8-10** (=10 / 1.3 + \"just-in-case\" margin) scans. E.g., with this combination of parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "* `next_batch`: `batch_size` = 10\n",
    "*  `sample_nodules`: `share` = 0.6\n",
    "* `sample_nodules`: `batch_size` = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "you will get the expected **12 (=20 * 0.6)** cancerous nodules and **8** noncancerous for most batches. In the same time, `batch_size=5` in `next_batch` is unlikely to yield **12** cancerous crops. In this case, `sample_nodules` just takes all cancerous crops it can find, and complements them with random crops to form a batch of expected size (**20**, in our example).  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building batches with only cancerous crops"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may sometimes want to generate batches containing only (and all) cancerous crops, that can be found in a batch, flowing into a pipeline. For that you only have to set `batch_size` to `None` and `share` to `1.0`:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "crops_sampling_pipeline = preprocessing.sample_nodules(batch_size=None, nodule_size=(16, 32, 32),\n",
    "                                                       share=1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, however, you will end up getting batches of different sizes in different runs of `next_batch`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "batch_crops_1 = (luna_dataset >> crops_sampling_pipeline).next_batch(4)\n",
    "batch_crops_2 = (luna_dataset >> crops_sampling_pipeline).next_batch(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(batch_crops_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(batch_crops_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that all crops contain cancer now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_of_cancerous_pixels(batch_crops_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Randomizing locations of cancer in crops"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a look one more time at a cancerous crop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_slices(batch_crops, 2, 8, components=['images', 'masks'], clims=[(-1200, 300), (0, 1)],\n",
    "            grid=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that cancer is located in the **center** of the crop. This is the default behaviour of `sample_nodules`. To avoid possible overfitting, `you` can introduce variability in the location of cancer inside crops by supplying `variance`-parameter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "crops_sampling_pipeline = preprocessing.sample_nodules(batch_size=None, nodule_size=(16, 32, 32),\n",
    "                                                       share=1.0, variance=(9, 36, 36))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that `variance` should always be a sequence of three numbers. Each number is a variance of a zero-mean normal distribution used for sampling locations of cancer inside crops. Try it out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "batch_crops = (luna_dataset >> crops_sampling_pipeline).next_batch(5, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_nodules_pixel_coords(batch_crops).head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_slices(batch_crops, 1, 4, components=['images', 'masks'], clims=[(-1200, 300), (0, 1)],\n",
    "            grid=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The basic rule of thumb is to chose `variance` in a way, so that squared root of `variance` wouldn't exceed the quarter of size of crop (`nodule_size`) along respective axis (read about [$2\\sigma$](https://en.wikipedia.org/wiki/68–95–99.7_rule))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Shortcut to RadIO capabilities: `pipelines`-submodule"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Up to this point we've covered a lot of actions (`unify_spacing`, `resize`, `sample_nodules`, `rotate`,...) that offer flexible preprocessing capabilities. Still, chaining them in a correct way in a workflow can be a challenge. To make life easier for you, RadIO-team prepared a `pipelines`-submodule, incroporating several ready-to-use workflows. These are: `get_crops` - a simple workflow, preparing batches of crops for training a neural net, and workflows `split_dump` and `combine_crops`, that allow for **very fast** generation of training examples for neural networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### `get_crops`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can make a fresh crops-generating workflow in one line of code. Do not forget to supply `nodules`-dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from radio.pipelines import get_crops\n",
    "crops_sampling = get_crops(nodules_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most importantly, `workflows` produced by `get_crops` contain actions `unify_spacing` and `sample_nodules`. RadIO team set the arguments of these actions to good [default values](link-on-default). However, nothing stops you from setting your own parameters. E.g., you may want to generate batches of **6** cancerous and **4** noncancerous crops of shape **[24, 48, 48]**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "crops_sampling = get_crops(nodules_df, share=0.6, batch_size=10, nodule_shape=(24, 48, 48))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check out batches generated by this workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_crops = (luna_dataset >> crops_sampling).next_batch(7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check that **six out of ten** crops are indeed cancerous:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_of_cancerous_pixels(batch_crops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_nodules_pixel_coords(batch_crops).head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_slices(batch_crops, 0, 9, components=['images', 'masks'], clims=[(0, 255), (0, 1)],\n",
    "            grid=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: we recommend to use `get_crops` only for testing purposes, as it generates no more than a couple of thousand of training examples in one pass through Luna-dataset (which takes several hours to complete). For production and research we encourage you to take a look at `split_dump` and `combine_crops`-workflows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating large datasets of crops with `split_dump`  and `combine_crops`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With `split_dump` you can generate (and dump on disk) more than **100000** training examples in one run through the Luna-dataset. Workflow `combine_crops` can then be used for loading balanced batches of generated crops. You should always use one pipeline after another, starting with `split_dump`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 1: `split_dump`**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As opposed to `get_crops`-workflow, that uses batches of several scans for sampling batches of **10-30** crops, `split_dump` samples **thousands** crops from each batch, and dumps them on disk. As a result, one run through Luna-dataset is all it takes to generate **100000** training examples. Importantly, `split_dump` dumps cancerous and noncancerous crops in *separate folders*. Keep it in mind, when you use the method to produce a workflow. Also, do not forget to supply Luna-annotations dataframe: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from radio.pipelines import split_dump\n",
    "cancerous_folder, non_cancerous_folder = '/data/crops_folder/cancer', '/data/crops_folder/noncancer'\n",
    "crops_dumping = split_dump(cancerous_folder, non_cancerous_folder, nodules_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can now `run` the whole dataset through the created workflow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# (luna_dataset >> crops_dumping).run()\n",
    "\n",
    "# running the whole dataset Luna through the workflow takes several hours\n",
    "# so, think thoroughly before uncommenting the first line and running the cell!\n",
    "# (crops will also take about 100GB of your disk-space)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At least for purposes of demonstration, it might be a better idea to `run` a small part of Luna through the workflow. Gladly, a `Dataset`-instance supports easy splitting on train-test-validation parts, just like that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "luna_dataset.split([0.8, 0.1, 0.1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You now have a train part of `Luna_dataset`, that has **80 %** of items from the whole Luna:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(luna_dataset.train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(luna_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For purposes of demonstration, we suggest you to pass an even smaller part through the workflow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "luna_dataset.split([0.03])\n",
    "len(luna_dataset.train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# may take a couple of minutes\n",
    "(luna_dataset.train >> crops_dumping).run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should now be able to see a lot of directories in `cancerous_folder` and `noncancerous_folder`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(os.listdir(cancerous_folder))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(os.listdir(non_cancerous_folder))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are crops, dumped with `dump`-action. Read the [documentation](https://analysiscenter.github.io/lung_cancer/intro/preprocessing.html#load-and-dump) to understand the content of folders and what `dump` does."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 2: `combine_crops`**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`combine-crops` allows you create batches, balancing cancerous and noncancerous crops in any way you want. The workflow always works with crops created by `split_dump`. So, the first step is to associate datasets with folders, created in previous section:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cancer_index = FilesIndex(path=os.path.join(cancerous_folder, '*'), dirs=True)\n",
    "non_cancer_index = FilesIndex(path=os.path.join(non_cancerous_folder, '*'), dirs=True)\n",
    "cancer_set = Dataset(cancer_index, batch_class=CTImagesMaskedBatch)\n",
    "non_cancer_set = Dataset(non_cancer_index, batch_class=CTImagesMaskedBatch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's it, you can create a fresh workflow using `combine_crops`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from radio.pipelines import combine_crops\n",
    "crops_sampling = combine_crops(cancer_set, non_cancer_set, batch_sizes=(5, 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pay attention to parameter `batch_sizes`. In the example above, it is set to **(5, 5)**. This means, that each batch generated by crops_sampling, will contain **5** cancerous and **5** noncancerous crops. See it yourself:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "batch_crops = crops_sampling.next_batch()\n",
    "# wow! This works really fast!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_of_cancerous_pixels(batch_crops)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may want to generate batches containing **8** cancerous crops and **2** noncancerous:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "crops_sampling = combine_crops(cancer_set, non_cancer_set, batch_sizes=(8, 2))\n",
    "batch_crops = crops_sampling.next_batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_of_cancerous_pixels(batch_crops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_slices(batch_crops, 9, 10, components=['images', 'masks'], clims=[(0, 255), (0, 1)],\n",
    "            grid=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### What's next?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After reading this tutorial you should be able to generate batches of crops with any type of balance using `sample_nodules`. You also should be able to use predefined workflow `split_dump` to **dump a large dataset of crops** on disk and then load balanced batches of cancerous/noncancerous crops in the blink of an eye with `combine_crops`.\n",
    "\n",
    "In the [4th](https://github.com/analysiscenter/radio/blob/master/tutorials/RadIO.IV.ipynb) (and last) tutorial you will be able to make use of all these crops and train a **segmenting [VNet](https://arxiv.org/abs/1606.04797)-like model on task of cancer-segmentation**."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
